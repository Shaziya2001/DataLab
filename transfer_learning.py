# -*- coding: utf-8 -*-
"""Transfer_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10jI6yolwcDgOXNQcF5fMaw7nZ6MqTy2d

#**Ripik.AI HackFest: Unleashing AI Potential**

**Dataset Description:**

The dataset consists of car images with various types of damages commonly encountered in insurance claims. Each image is labeled with a specific type of damage, such as cracks, scratches, flat tires, dents, shattered glass, and broken lamps. The goal is to develop a computer vision model that can automatically classify these images, aiding insurance companies in identifying and preventing exaggerated or false claims. The training set, provided in the `train.zip` file, contains images in the 'images' folder and is accompanied by a 'train.csv' file with columns specifying image_id, filename, and corresponding damage labels.

Additionally, a test set is provided in the 'test.zip' file for evaluating the model's performance. The test set contains images to predict.

**Objective:**

Develop a robust image classification model using DenseNet201 for accurately identifying and categorizing car damages. Achieve high precision, recall, and F1-score to enhance the efficiency of insurance claim processing.

**Interpretation:**

The model achieved a high accuracy of 96.25%, demonstrating its effectiveness in classifying car damage types. Precision, recall, and F1-score metrics indicate strong performance across most damage categories. There's potential for improvement in precision for the "lamp broken" category. Overall, the model holds promise for automating and improving the accuracy of insurance claim assessments.
"""

#Importing the required libraries
import zipfile
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.applications import DenseNet201
from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.densenet import preprocess_input
from tensorflow.keras.preprocessing.image import img_to_array, load_img

from google.colab import drive
drive.mount('/content/drive')

#Loading the zipfile
with zipfile.ZipFile('/content/drive/MyDrive/Ripik Hackathon Dataset/train.zip', 'r') as zip_ref:
    zip_ref.extractall('train')

#Loading the training dataset
train_df = pd.read_csv('/content/train/train/train.csv')

#Converting the 'label' column to strings
train_df['label'] = train_df['label'].astype(str)

#Data Preprocessing
image_size = (224, 224)
batch_size = 16

#Splitting the dataset into training and validation sets
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

#Creating ImageDataGenerators for training and validation sets
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1.0/255.0)

train_generator = train_datagen.flow_from_dataframe(
    train_df,
    directory='/content/train/train/images',
    x_col='filename',
    y_col='label',
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True
)

val_generator = val_datagen.flow_from_dataframe(
    val_df,
    directory='/content/train/train/images',
    x_col='filename',
    y_col='label',
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False
)

# Generate augmented images and labels
images, labels = val_generator.next()
#Plot the augmented images
plt.figure(figsize=(10, 10))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(images[i])
    plt.title(labels[i])
    plt.axis('off')
plt.show()

#Loading the DenseNet201 model pre-trained on ImageNet data
base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
#Freeze the pre-trained layers
for layer in base_model.layers[-20:]:
    layer.trainable = False
#Building the model
model=Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(6, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])
#Display the model summary
model.summary()

#Model training
history=model.fit(train_generator,epochs=20,validation_data=val_generator)

predictions=model.predict(val_generator)
y_true=val_generator.classes
#Convert predictions to class labels
y_pred=np.argmax(predictions,axis=1)
accuracy=accuracy_score(y_true,y_pred)
class_report=classification_report(y_true,y_pred)
print("Classification Report:")
print(class_report)
print("\nAccuracy Score:")
print(accuracy)

#Loading the test zipfile
with zipfile.ZipFile('/content/drive/MyDrive/Ripik Hackathon Dataset/test.zip', 'r') as zip_ref:
    zip_ref.extractall('test')

#Loading the testing dataset
test_df = pd.read_csv('/content/test/test/test.csv')
#Create a data generator for test data
test_datagen = ImageDataGenerator(rescale=1.0/255.0)
test_generator = test_datagen.flow_from_dataframe(
    test_df,
    x_col='filename',
    y_col=None,
    directory='/content/test/test/images',
    target_size=(224, 224),
    batch_size=128,
    class_mode=None,
    shuffle=False
)
#Making predictions for the test set
predictions = model.predict(test_generator)
#Get the predicted labels
predicted_labels = tf.argmax(predictions, axis=1).numpy() + 1
#Creating a submission DataFrame
submission_df = pd.DataFrame({'image_id': test_df['image_id'], 'label': predicted_labels})
#Saving the submission DataFrame to a CSV file
submission_df.to_csv('submission.csv', index=False)